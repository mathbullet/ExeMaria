[
  {
    "objectID": "aisuite-tutorial.html",
    "href": "aisuite-tutorial.html",
    "title": "aisuite — 複数のLLMを最小の労力で動かす",
    "section": "",
    "text": "このノートブックでは、LLMとのCUIベースの対話において、LLMプロバイダやモデルの切り替えにかかる労力を最小化するライブラリ、aisuiteについて説明します。"
  },
  {
    "objectID": "aisuite-tutorial.html#背景",
    "href": "aisuite-tutorial.html#背景",
    "title": "aisuite — 複数のLLMを最小の労力で動かす",
    "section": "背景",
    "text": "背景\nHuggingFace TransformersやLangChainといった既存のライブラリを使ってLLMにプロンプトを入力し、出力結果を得ることは可能です。しかし、これらのライブラリは使用するLLMのモデルやプロバイダによってインポートすべきクラスや必要な前後処理が異なる場合があり、これが若干の手間になっています。\n例えばLangChain v.0.3では、LLMのプロバイダごとに異なるパッケージが用意されています。そのため、プロバイダごとにパッケージをインストールし、プロバイダごとに異なるコードでLLMと対話することになります。Anthropic、OpenAIのモデルの出力を比較をしたい、という時は、以下のような手順を踏む必要があります。\npip install \\\n    langchain \\\n    langchain-anthropic \\\n    langchain-openai\n\n# AnthropicのClaude 3 Opusを使用する場合\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-opus-20240229\")\nresponse = model.invoke(\"こんにちは！あなたは誰ですか？\")\nprint(response.content)\n\nはじめまして！私はAnthropic社が開発した人工知能アシスタントのClaudeです。人間とのコミュニケーションを通じて、様々なことを学び、人々のお手伝いをすることが私の役割です。知識の提供から創造的なタスクまで、幅広い分野でサポートができます。よろしくお願いします！\n\n\n\n# OpenAIのGPT-4oを使用する場合\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o\")\nresponse = model.invoke(\"こんにちは！あなたは誰ですか？\")\nprint(response.content)\n\nこんにちは！私はOpenAIが開発したAIアシスタントです。何かお手伝いできることがありますか？\n\n\nここでは実行はしませんが、HuggingFaceのモデルを使う場合はさらに差分が大きくなります。\npip install langchain-huggingface\nfrom langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n\nllm = HuggingFaceEndpoint(\n    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n    max_new_tokens=256,\n)\n\nmodel = ChatHuggingFace(llm=llm)\nresponse = model.invoke(\"こんにちは！あなたは誰ですか？\")\nprint(response.content)\n毎度使うモデルやプロバイダが変わるたびにコードの書き心地が変わるより、全て統一のインターフェースで使える方がいいよね、というのがaisuiteのモチベーションです。"
  },
  {
    "objectID": "aisuite-tutorial.html#aisuite",
    "href": "aisuite-tutorial.html#aisuite",
    "title": "aisuite — 複数のLLMを最小の労力で動かす",
    "section": "aisuite",
    "text": "aisuite\nそれではaisuiteを触ってみましょう。以下コマンドでインストールします。\npip install 'aisuite[all]'\n先ほどと同じように、複数プロバイダのLLMを使用するとき、プロバイダごとに別途用意する必要があるのは{プロバイダ名}:{モデル名}という形式の文字列のみです。\n\nclaude3opus = \"anthropic:claude-3-opus-20240229\"\ngpt4o = \"openai:gpt-4o\"\n# zephyr = \"huggingface:HuggingFaceH4/zephyr-7b-beta\"\n\nどのプロバイダのどのモデルを使うとしても、モデルからの応答を得る方法は同じです。まずはClientというクラスのインスタンスを作成します。\n\nimport aisuite\n\nclient = aisuite.Client()\n\n併せて、モデルに入力するメッセージも作成しておきます。\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"こんにちは！あなたは誰ですか？\"}\n]\n\nそして、client.chat.completions.createというメソッドによってメッセージの送信を行います。\nClaudeを使う場合は以下のようになります。\n\nresponse = client.chat.completions.create(\n        model=claude3opus,\n        messages=messages\n    )\nprint(response.choices[0].message.content)\n\nはじめまして！私はAnthropic社が開発した人工知能のアシスタントです。クロードと申します。\n\n人間との会話を通じて、知識や情報の提供、アイデアの提案、様々なタスクのサポートなどを行うことを目的としています。学習を重ねることで、できるだけ自然で有益な対話を心がけています。\n\nただし、私はあくまでもAIであり、人間のように感情を持ったり、深い関係性を築いたりすることはできません。私にできることと、できないことの区別を意識しながら、皆さまのお役に立てるよう努力したいと思います。どうぞよろしくお願いいたします。\n\n\nGPT-4oを使う場合は以下のようになります。\n\nresponse = client.chat.completions.create(\n        model=gpt4o,\n        messages=messages\n    )\nprint(response.choices[0].message.content)\n\nこんにちは！私はAIアシスタントです。あなたの質問に答えたり、情報を提供したりするためにここにいます。何か手伝えることがありますか？\n\n\nここで、両者が異なるのはmodelの値のみである点に注目してください。プロバイダやモデルを変えたい場合はここを書き換えるだけでよく、差分が最小化されていることがわかります。\nLLMの性能比較を行う際など、複数のLLMを呼び出して応答を得たい場面で有用ではないかと思います。また、シンプルに綺麗なインターフェースで使いやすいので、さらに機能が追加されて活用できる場面が広がると嬉しいですね。"
  },
  {
    "objectID": "aisuite-tutorial.html#補足",
    "href": "aisuite-tutorial.html#補足",
    "title": "aisuite — 複数のLLMを最小の労力で動かす",
    "section": "補足",
    "text": "補足"
  }
]